\documentclass[11pt,a4paper]{report}
%\fontfamily{cmss}
\hfuzz=9999pt % "fix" hbox overfull
%Path relative to the .tex file containing the \includegraphics command
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue
}

\begin{titlepage}
\title{BU551V - Applied Econometrics \\ Lecture Notes}
\author{Rodrigo Miguel}
\date{\today}
\end{titlepage}



\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction to Applied Econometrics}
\section{What is statistics?}
Statistics is the science of uncertainty.
\begin{itemize}
    \item What \textit{could} be;
    \item What \textit{might} be;
    \item What \textit{probably} is.
\end{itemize}
Statistics can be divided into two:
\begin{itemize}
    \item \textbf{Descriptive statistics:} Used to summarize information (how many), describe in a concise and easy to understand way.
    \item \textbf{Inferential statistics:} Asks about what we can learn from the population in the sample.
\end{itemize}
\subsection{Example}
If an airline is selling tickets to a flight with only 100 seats. A safe solution is to accept only 100 reservations. However, due to the human nature, cancellations and no-show-ups might happen. What would be the optimal number of reservations the airline should make?

\section{Statistical Inference}
\subsection{Statistical Inference}
Is used when we want to learn about a population given a sample/subset.
\subsection{Steps to Make a Statistical Inference}
\begin{itemize}
    \item Identify the population of interest;
    \item Specify a model for the population relationship of interest that contains unknown parameters.
    \item Obtain a \textbf{random} sample from the population.
    \item Estimate the parameters using the sample.
\end{itemize}
\subsection{Random Sample}
A \textbf{Random Sample} is a set of random variables (RVs) that follow a probability density function (pdf) $f(y,\theta)$.
\\When the RVs are from a random sample with $f(y,\theta)$ density, they are said to be \textbf{independent and identically distributed}.
\\Random samples can be assumed to be drawn from a normal distribution. Its population can be characterized by two parameters:
\begin{itemize}
    \item Mean $\mu$;
    \item Variance $\sigma^2$.
\end{itemize}
Usually we are only interested in $\mu$, however, in some cases, we also need to know $\sigma^2$.
\subsection{Estimators}
An estimator is a rule that assigns a value of $\theta$ to the sample.
\\For an actual realized sample, the estimate is just the average in the sample:
\[\overline{y} = \frac{(y_1 + y_2 + ... + y_n)}{n}\]

\subsection{General Expression for Estimators}
An estimator \textit{W} of a parameter $\theta$ can be expressed as an abstract formula:
\[W = h(Y_1, Y_2, ..., Y_n)\], where \textit{h} is an unknown function.
\\When we plug a set of actual observations ${y_1, y_2, ... y_n}$ into the function h, we obtain an estimate of $\theta$.

\section{Choosing an estimator}
To choose an estimator, criteria needs to be followed so that an estimator with "desirable" properties is correctly chosen. Here's the several properties that are used as choosing criteria:
\begin{itemize}
    \item Unbiasedness;
    \item Efficiency;
    \item Consistency.
\end{itemize}

\section{Unbiasedness}
An estimator of $\theta$ is \textbf{unbiased} if: \[E(W) = \theta\]
This does, however, \textbf{not} mean that an estimate in a particular sample is equal to $\theta$.
\\It means that:
\begin{enumerate}
    \item We draw random samples from the population many times;
    \item Compute an estimate each time;
    \item Average these estimates over all random samples;
    \item Then we obtain the true parameter $\theta$.
\end{enumerate}

\subsection{Bias of an estimator}
An estimator of $\theta$, W, is a biased estimator if:
\[E(W) \neq \theta\]
Its bias is defined as:
\[Bias(W) \equiv E(W) - \theta\]

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/unbiasedness.png}
    \caption{An unbiased estimator W, and an estimator with positive bias, $W_2$.}
    \label{Unbiasedness}
\end{figure}

\begin{itemize}
    \item An estimator being unbiased does not necessarily mean it's a good estimator.
\end{itemize}

\section{Efficiency}
\subsection{Sampling Variance of Estimators}
\begin{itemize}
    \item Unbiasedness ensures that the distribution of an estimator has a mean value which is equal to the true parameter $\theta$.
    \item It is a good property but not enough.
    \item We want an estimator that can show us a mean value equal to $\theta$ and centred around $\theta$ as tightly as possible.
\end{itemize}
To measure how spread out a distribution is, we use the variance, Var(W). To calculate it, we use the sample average as an estimator $w = \overline{Y}$.
\paragraph{Summary}
If ${Y_1, ..., Y_n}$ is a random sample from a population with mean $\mu$ and variance $\sigma^2$:
\begin{itemize}
    \item $\overline{Y}$ has a mean $\mu$ and variance $\frac{\sigma^2}{n}$
\end{itemize}
\subsection{Which estimator is better?}
Among unbiased estimators, we prefer the estimator with the smallest variance.

\section{Consistency}
Consistency, is a property that captures how estimators improve as the sample size, \textit{n}, increases.
\subsection{Example}
\begin{itemize}
    \item An estimator $\overline{Y}$ has $Var(\overline{Y}) = \frac{\sigma^2}{n}$, meaning that as \textit{n} gets larger, the estimator improves.
    \item An estimator $Y_1$ has $Var(Y_1) = \sigma^2$, meaning no improvement even if \textit{n} gets larger.
\end{itemize}
\subsection{Importance of Consistency}
Consistency is a minimum requirement of an estimator used in econometrics.
\subsection{Law of Large Number}
The result that $\overline{Y}$ is consistent for $\mu$  is known as the \textbf{law of large numbers} (LLN):
\[plim(\overline{Y}) = \mu\]

\chapter{Point Estimation}
\section{Terminology and Notation}
\subsection{Estimator vs. Estimate}
\begin{itemize}
    \item Estimator is \underline{a formula};
    \item Estimate is \underline{a value}.
    \item Suppose {$Y_1$, ..., $Y_n$} is a random sample from the population with mean $\mu$.
    \begin{itemize}
        \item An example of an \textbf{estimator} is the sample average $\overline{Y}$:
        \[ \overline{Y} = \frac{1}{n} \displaystyle\sum_{i=1}^{n} Y_i \]
        \item Using a sample, the actual value of $\overline{Y}$ can be calculated.
        \item Using this, \[\overline{y} = \frac{1}{3}(16 + 8 + 21) = 15\], \textit{15} is the \textbf{estimate}.
    \end{itemize}
\end{itemize}
\subsection{Notations}
\begin{itemize}
    \item \underline{Upper case} is used to denote an estimator.
    \begin{itemize}
        \item Which itself is a random variable.
        \item Example: $W$, $\overline{Y}$, $S_X$, $S_{XY}$. 
    \end{itemize}
    \item After obtaining a sample, you can calculate an estimate.
    \item \underline{Lower case} is used to denote an estimate.
    \begin{itemize}
        \item Example: $w$, $overline{y}$, $s_X$, $s_{XY}$.
    \end{itemize}
\end{itemize}
\section{Maximum Likelihood and Least Squares}
\subsection{Maximum likelihood}
\begin{itemize}
    \item Suppose {$Y_1$, ... $Y_n$} is a random smample from the population distribution, f(Y; $\theta$).
    \item As Yi is a random sample from f(y;$\theta$), the joint distribution of {$Y_1$, ..., $Y_n$} is:
    \[f(Y_1;\theta) \times f(Y_2;\theta) \times ... \times f(Y_n; \theta)\]
    \item Using this, define the likelihood function as:
    \[L(\theta; Y_1, ..., Y_n) = f(Y_1;\theta) \times f(Y_2; \theta) \times ... \times f(Y_n; \theta)\]
    \item The maximum likelihood estimator (MLE) of $\theta$, call it W, chooses the value of $\theta$ that maximises the likelihood function.
    \item For a practical reason, it is more convenient to work with the log-likelihood function.
    \item The MLE is known to be:
    \begin{itemize}
        \item Consistent and sometimes unbiases;
        \item Asymptotically efficient.
    \end{itemize}
    \item We will need maximum likelihood to estimate the parameters of advanced econometric models.
\end{itemize}
\subsection{Least Squares}
\begin{itemize}
    \item The second estimator is called \textbf{least squares estimator};
    \item Suppose {$Y_1$, ..., $Y_n$ is a random sample from the population iwth mean $\mu$};
    \item Objective - We want to estimate $\mu$ using the sample;
    \item How? - The least squares estimator choose the value of \textit{m} that makes the sum of squared deviations as small as possible;
    \item Solving this for \textbf{m} gives:
    \[m = \frac{1}{n}\sum_{i=1}^{n}Y_i = \overline{Y}\]
    \begin{itemize}
        \item whch is simply the sample average.
    \end{itemize}
    \item For many distributions, such as the normal distribution, the sample average $\overline{Y}$ is also the MLE of $\mu$;
    \item Thus, the principles of least squares and maximum likelihood often result in the same estimator.
\end{itemize}
\section{Method of Moments}
\begin{itemize}
    \item Usually, the parameter to be estimated, $\theta$, is shown to be related to some expected value in the distribution of $Y$, such as $E(Y)$. Using this, the method of moments estimation proceeds as follows.
    \item Suppose that $\theta$ is related to the population mean as:
    \[\theta = g(\mu)\] for some function g.
    \item To estimate $\theta$, it is natural to replace $\mu$ with $\overline{Y}$, which gives us the estimator of $\mu$:
    \[g(\overline{Y})\]
    \item Summary - What we are doing is replacing the population moment, $\mu$, with its sample counterpart, $\overline(Y)$.
\end{itemize}
\subsection{Sample Covariance}
\begin{itemize}
    \item The covariance between two random variables, X and Y, is defined as:
    \[\sigma_{XY} = E[(X-\mu_X)(Y - \mu_Y)]\]
    \item Method of moments suggests replacing population moments with their sample counterparts:
    \[\sum_{i=1}^{n}\frac{(X_i - \overline{X})(Y_i-\overline{Y})}{n} = \frac{1}{n}\times \sum_{i=1}^{n}(X_i - \overline{X})(Y_i - \overline{Y})\]
    \begin{itemize}
        \item This is a consistent estimator of $\sigma_{XY}$.
    \end{itemize}
    \item It can be shown (even though we don't) that replacing $n$ with $n-1$ yields an unbiased (and consistent) estimator of $\sigma_{XY}$:
    \[S_{XY} = \frac{1}{n-1} \times \sum_{i=1}^{n} (X_i - \overline{X})(Y_i - \overline{Y})\]
\end{itemize}
\subsection{Sample correlation coefficient}
\begin{itemize}
    \item The population correlation is defined as:
    \[\rho = \frac{\sigma_{XY}}{\sigma_X\sigma_Y}\]
    \item The method of moments suggests estimating $\rho_{XY}$ by replacing population moments with their sample counterparts;
    \begin{itemize}
        \item Called "sample correlation coefficient".
    \end{itemize}
\end{itemize}


\end{document}